{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92eb223",
   "metadata": {},
   "source": [
    "# Modelos 3\n",
    "### Uso de regressão L1 e L2\n",
    "Nome: Joaquim Junior e Matheus Velloso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f07f2",
   "metadata": {},
   "source": [
    "#### 1. Introdução\n",
    "Em Machine Learning, a regressão linear é um dos modelos mais simples de previsão de dados. Porém, ela pode sofrer com o Underfit e Overfit. O que acontece é que a regressão linear faz previsões calculando um polinômio para tentar representar o resultado. Porém, para representar com grande acurácia os dados de treino, é necessário um polinômio de grau alto, que pode gerar resultados melhores para os dados de treino, mas piores para os demais, ou seja overfit (super ajustado). Veja na imagem abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71081cd6",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"https://analystprep.com/study-notes/wp-content/uploads/2021/03/Img_13.jpg\" alt=\"Fern vs Ehre\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad611f51",
   "metadata": {},
   "source": [
    "Pensando nisso, foram desenvolvidos modelos de regularização do grau desses polinômios para evitar o Overfit, como é o caso do L1 (LASSO) e L2 (Ridge), que serão estudados a seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f93a03",
   "metadata": {},
   "source": [
    "### 2. Teoria\n",
    "Ambos os métodos tentam diminuir o efeito de Overfitting da regressão. Primeiramento relembremos o cálculo do MSE:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d1a7c",
   "metadata": {},
   "source": [
    "$$\n",
    "MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cf7eb9",
   "metadata": {},
   "source": [
    "Suponha que o valor a ser predito seja um polinômio tal que:\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2^2 + \\theta_3 x_3^3 + \\theta_4 x_4^4...$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983986be",
   "metadata": {},
   "source": [
    "Substituindo:\n",
    "$$\n",
    "MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - (\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2^2 + \\theta_3 x_3^3 + \\theta_4 x_4^4...))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0012e1",
   "metadata": {},
   "source": [
    "Chamemos o polinômio de f, para simplificar\n",
    "$$\n",
    "MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - f(x_i))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9d8909",
   "metadata": {},
   "source": [
    "Para diminuir o grau do polinômio, a regularização acrescenta um demérito para graus maiores. Veja como é feito em L2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a8281",
   "metadata": {},
   "source": [
    "$$\n",
    "MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - f(x_i))^2 + \\alpha\\sum_{i=1}^{n}(\\theta_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ab7bc4",
   "metadata": {},
   "source": [
    "Ou seja, quanto maior n, maior o somatório dos quadrados de theta, sendo vantajoso um modelo que tenha menor grau e boa precisão. Alpha é uma constante que pode ser escolhida para ditar o peso do demérito."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920e0e1",
   "metadata": {},
   "source": [
    "Agora, vejamos como é feito em L1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69129cf3",
   "metadata": {},
   "source": [
    "$$\n",
    "MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - f(x_i))^2 + \\alpha\\sum_{i=1}^{n}|\\theta_i|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd1422c",
   "metadata": {},
   "source": [
    "Ele também adiciona um demérito para o aumento de grau, mas agora usando o módulo de theta com a constante alpha.\n",
    "\n",
    "Por fim, o Elastic-Net é o efeito de L1 e L2 juntos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499a6cc2",
   "metadata": {},
   "source": [
    "$$\n",
    "MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - f(x_i))^2 + \\alpha(l_1ratio)\\sum_{i=1}^{n}|\\theta_i| + 0.5\\alpha(1-l_1ratio)\\sum_{i=1}^{n}(\\theta_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbadd6e",
   "metadata": {},
   "source": [
    "### 3. Uso\n",
    "#### 3.1 Importação e Tratamento do Dataframe\n",
    "Usarei o data set student_exam_scores, disponível na plataforma Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b507cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e2bc3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hours_studied</th>\n",
       "      <th>sleep_hours</th>\n",
       "      <th>attendance_percent</th>\n",
       "      <th>previous_scores</th>\n",
       "      <th>exam_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>72.1</td>\n",
       "      <td>45</td>\n",
       "      <td>30.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.3</td>\n",
       "      <td>8.6</td>\n",
       "      <td>60.7</td>\n",
       "      <td>55</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>73.7</td>\n",
       "      <td>86</td>\n",
       "      <td>35.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>95.1</td>\n",
       "      <td>66</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.1</td>\n",
       "      <td>6.4</td>\n",
       "      <td>89.8</td>\n",
       "      <td>71</td>\n",
       "      <td>40.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>10.5</td>\n",
       "      <td>5.4</td>\n",
       "      <td>94.0</td>\n",
       "      <td>87</td>\n",
       "      <td>42.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>7.1</td>\n",
       "      <td>6.1</td>\n",
       "      <td>85.1</td>\n",
       "      <td>92</td>\n",
       "      <td>40.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>63.8</td>\n",
       "      <td>76</td>\n",
       "      <td>28.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>12.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>50.5</td>\n",
       "      <td>58</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>10.2</td>\n",
       "      <td>6.3</td>\n",
       "      <td>97.4</td>\n",
       "      <td>68</td>\n",
       "      <td>37.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     hours_studied  sleep_hours  attendance_percent  previous_scores  \\\n",
       "0              8.0          8.8                72.1               45   \n",
       "1              1.3          8.6                60.7               55   \n",
       "2              4.0          8.2                73.7               86   \n",
       "3              3.5          4.8                95.1               66   \n",
       "4              9.1          6.4                89.8               71   \n",
       "..             ...          ...                 ...              ...   \n",
       "195           10.5          5.4                94.0               87   \n",
       "196            7.1          6.1                85.1               92   \n",
       "197            1.6          6.9                63.8               76   \n",
       "198           12.0          7.3                50.5               58   \n",
       "199           10.2          6.3                97.4               68   \n",
       "\n",
       "     exam_score  \n",
       "0          30.2  \n",
       "1          25.0  \n",
       "2          35.8  \n",
       "3          34.0  \n",
       "4          40.3  \n",
       "..          ...  \n",
       "195        42.7  \n",
       "196        40.4  \n",
       "197        28.2  \n",
       "198        42.0  \n",
       "199        37.8  \n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importar e tratar dataset\n",
    "df = pd.read_csv(\"student_exam_scores.csv\")\n",
    "df = df.iloc[0:, 1:]\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51598202",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"exam_score\", axis=1)\n",
    "y = df[\"exam_score\"]\n",
    "SEMENTE = 27\n",
    "#Divisão treino teste\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.1, random_state=SEMENTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0c53a6",
   "metadata": {},
   "source": [
    "#### 3.2 Regressão Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0625155",
   "metadata": {},
   "source": [
    "Façamos agora uma Regressão Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aab89dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O score nos casos de teste foram 0.7254087630089416 enquanto no de treino 0.8455916021138415.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression().fit(train_x, train_y)\n",
    "print(f\"O score nos casos de teste foram {reg.score(test_x, test_y)} enquanto no de treino {reg.score(train_x, train_y)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03c1381",
   "metadata": {},
   "source": [
    "O modelo obteve um score de 0.725 nos dados de teste. Tentaremos aplicar agora os regularizadores LASSO e Ridge e usarmos a regressão linear como padrão para compararmos suas eficiências."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102a1a41",
   "metadata": {},
   "source": [
    "#### 3.3 Regressão L1 (LASSO)\n",
    "Façamos agora a regressão L1 para observarmos se de fato podemos aumentar a acuracia. Para esse exemplo usei alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba6c9738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O score nos casos de teste foram 0.7609801969550539 enquanto no de treino 0.8341574289953586.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "lasso_reg = linear_model.Lasso(alpha=1)\n",
    "lasso_reg.fit(train_x, train_y)\n",
    "\n",
    "print(f\"O score nos casos de teste foram {lasso_reg.score(test_x, test_y)} enquanto no de treino {lasso_reg.score(train_x, train_y)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6bb076",
   "metadata": {},
   "source": [
    "Veja que a acuracia aumentou significativamente, de 0.725 para 0.761 com baixa diferença nos casos de treino. Vejamos como varia a acurácia para diferentes alpha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58a8e71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7304482642755346, 0.7609801969550539, 0.7635921687691649, 0.7334644511650309, 0.5135731234012768, -0.3005492479741043, -0.3005492479741043]\n",
      "[0.8454772481182318, 0.8341574289953586, 0.7998553275828353, 0.7442300774513069, 0.5768581614695175, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "#Vejamos como varia com alpha:\n",
    "scores_test = []\n",
    "scores_train = []\n",
    "for alp in [0.1, 1, 2, 5, 10, 50, 200]:\n",
    "    lasso_reg = linear_model.Lasso(alpha=alp)\n",
    "    lasso_reg.fit(train_x, train_y)\n",
    "    scores_test.append(lasso_reg.score(test_x, test_y))\n",
    "    scores_train.append(lasso_reg.score(train_x, train_y))\n",
    "print(scores_test)\n",
    "print(scores_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f8ac74",
   "metadata": {},
   "source": [
    "Portanto, o desempenho do algoritmo depende crucialmente do alpha escolhido, podendo ser visto que para os da lista, o com maior desempenho foi alpha = 1 ou 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515bbe25",
   "metadata": {},
   "source": [
    "#### 3.4 Regressão L2 (RIDGE)\n",
    "Farei agora a regressão L2 com um alpha = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1f228d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O score nos casos de teste foram 0.7615529516656562 enquanto no de treino 0.8357295946214666.\n"
     ]
    }
   ],
   "source": [
    "ridge_reg = linear_model.Ridge(alpha=200)\n",
    "ridge_reg.fit(train_x, train_y)\n",
    "print(f\"O score nos casos de teste foram {ridge_reg.score(test_x, test_y)} enquanto no de treino {ridge_reg.score(train_x, train_y)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2893c035",
   "metadata": {},
   "source": [
    "Veja que a acuracia também pôde aumentar de forma significativa usando o Ridge, de 0.725 para 0.761 com também baixa diferença nos casos de treino. Variemos alpha agora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f47293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.725441519317793, 0.7257353569675498, 0.7260598074319422, 0.7270204555595647, 0.7285801114372805, 0.7393871256640926, 0.7615529516656562]\n",
      "[0.8455915979718392, 0.8455911891806184, 0.8455899559760613, 0.8455814172622392, 0.8455515346532589, 0.8447066959900845, 0.8357295946214666]\n"
     ]
    }
   ],
   "source": [
    "#Vejamos como varia com alpha:\n",
    "scores_test = []\n",
    "scores_train = []\n",
    "for alp in [0.1, 1, 2, 5, 10, 50, 200]:\n",
    "    lasso_reg = linear_model.Ridge(alpha=alp)\n",
    "    lasso_reg.fit(train_x, train_y)\n",
    "    scores_test.append(lasso_reg.score(test_x, test_y))\n",
    "    scores_train.append(lasso_reg.score(train_x, train_y))\n",
    "print(scores_test)\n",
    "print(scores_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c095d829",
   "metadata": {},
   "source": [
    "É possível perceber que alpha varia de modo bem diferente em L1 e L2, mas para ambos apresentam grande diferença. Para L2, o melhor alpha foi 200."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d656bf50",
   "metadata": {},
   "source": [
    "#### 3.5 Regressão L1 e L2 (Elastic-Net)\n",
    "Por fim, usaremos o Elastic-Net, que combina L1 e L2. Farei com alpha = 1 e divisão entre L1 e L2 de 0.7 para 0.3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56b9ceb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O score nos casos de teste foram 0.7610673614360217 enquanto no de treino 0.8362475907608985.\n"
     ]
    }
   ],
   "source": [
    "en_reg = linear_model.ElasticNet(alpha=1, l1_ratio=0.6)\n",
    "en_reg.fit(train_x, train_y)\n",
    "print(f\"O score nos casos de teste foram {en_reg.score(test_x, test_y)} enquanto no de treino {en_reg.score(train_x, train_y)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0570faaf",
   "metadata": {},
   "source": [
    "Veja que foi possível, assim como em L1 e L2, melhorar significativamente a acuracia, porém com agora uma gama muito maior de opções para isso, já que combina ambos os modelos vistos. Vejamos como varia o score variando l1_ratio e alpha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c0bfcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para l1_ratio=0.2:\n",
      "[0.7308697787905603, 0.7603734860296405, 0.7683518757142237, 0.7288368969972976, 0.6065087301991618, 0.054396902451969265, -0.26000739133353323]\n",
      "[0.8454690703978525, 0.8371160678186716, 0.8204086173235896, 0.7582440415724203, 0.6569361042941235, 0.29038432658386426, 0.03391541039642132]\n",
      "Para l1_ratio=0.4:\n",
      "[0.7308697787905603, 0.7603734860296405, 0.7683518757142237, 0.7288368969972976, 0.6065087301991618, 0.054396902451969265, -0.26000739133353323, 0.7307668379672224, 0.7607948755950209, 0.7695377873931322, 0.7259577506872463, 0.5932028878739339, -0.14335186769390051, -0.3005492479741043]\n",
      "[0.8454690703978525, 0.8371160678186716, 0.8204086173235896, 0.7582440415724203, 0.6569361042941235, 0.29038432658386426, 0.03391541039642132, 0.8454722660445787, 0.8367933398006434, 0.8184946060846876, 0.7468134198318727, 0.641620383863486, 0.15100701101451464, 0.0]\n",
      "Para l1_ratio=0.5:\n",
      "[0.7308697787905603, 0.7603734860296405, 0.7683518757142237, 0.7288368969972976, 0.6065087301991618, 0.054396902451969265, -0.26000739133353323, 0.7307668379672224, 0.7607948755950209, 0.7695377873931322, 0.7259577506872463, 0.5932028878739339, -0.14335186769390051, -0.3005492479741043, 0.7307147592536674, 0.7609519046407104, 0.7697958114538056, 0.7240460013135885, 0.5860426077358662, -0.16153128695044772, -0.3005492479741043]\n",
      "[0.8454690703978525, 0.8371160678186716, 0.8204086173235896, 0.7582440415724203, 0.6569361042941235, 0.29038432658386426, 0.03391541039642132, 0.8454722660445787, 0.8367933398006434, 0.8184946060846876, 0.7468134198318727, 0.641620383863486, 0.15100701101451464, 0.0, 0.8454735798619243, 0.8365523198476277, 0.8170641119908056, 0.7413156344373539, 0.6347134758998116, 0.12953589133547527, 0.0]\n",
      "Para l1_ratio=0.6:\n",
      "[0.7308697787905603, 0.7603734860296405, 0.7683518757142237, 0.7288368969972976, 0.6065087301991618, 0.054396902451969265, -0.26000739133353323, 0.7307668379672224, 0.7607948755950209, 0.7695377873931322, 0.7259577506872463, 0.5932028878739339, -0.14335186769390051, -0.3005492479741043, 0.7307147592536674, 0.7609519046407104, 0.7697958114538056, 0.7240460013135885, 0.5860426077358662, -0.16153128695044772, -0.3005492479741043, 0.7306622743697585, 0.7610673614360217, 0.7697419697060274, 0.72600115949027, 0.5771002887202661, -0.1804770859180842, -0.3005492479741043]\n",
      "[0.8454690703978525, 0.8371160678186716, 0.8204086173235896, 0.7582440415724203, 0.6569361042941235, 0.29038432658386426, 0.03391541039642132, 0.8454722660445787, 0.8367933398006434, 0.8184946060846876, 0.7468134198318727, 0.641620383863486, 0.15100701101451464, 0.0, 0.8454735798619243, 0.8365523198476277, 0.8170641119908056, 0.7413156344373539, 0.6347134758998116, 0.12953589133547527, 0.0, 0.8454747023460183, 0.8362475907608985, 0.8152000838791708, 0.7418670705588496, 0.6266599848763694, 0.1080901272327599, 0.0]\n",
      "Para l1_ratio=0.8:\n",
      "[0.7308697787905603, 0.7603734860296405, 0.7683518757142237, 0.7288368969972976, 0.6065087301991618, 0.054396902451969265, -0.26000739133353323, 0.7307668379672224, 0.7607948755950209, 0.7695377873931322, 0.7259577506872463, 0.5932028878739339, -0.14335186769390051, -0.3005492479741043, 0.7307147592536674, 0.7609519046407104, 0.7697958114538056, 0.7240460013135885, 0.5860426077358662, -0.16153128695044772, -0.3005492479741043, 0.7306622743697585, 0.7610673614360217, 0.7697419697060274, 0.72600115949027, 0.5771002887202661, -0.1804770859180842, -0.3005492479741043, 0.7305560842846655, 0.7611500156835241, 0.7682547980200298, 0.7297931236681865, 0.5522541833390004, -0.23881934887669432, -0.3005492479741043]\n",
      "[0.8454690703978525, 0.8371160678186716, 0.8204086173235896, 0.7582440415724203, 0.6569361042941235, 0.29038432658386426, 0.03391541039642132, 0.8454722660445787, 0.8367933398006434, 0.8184946060846876, 0.7468134198318727, 0.641620383863486, 0.15100701101451464, 0.0, 0.8454735798619243, 0.8365523198476277, 0.8170641119908056, 0.7413156344373539, 0.6347134758998116, 0.12953589133547527, 0.0, 0.8454747023460183, 0.8362475907608985, 0.8152000838791708, 0.7418670705588496, 0.6266599848763694, 0.1080901272327599, 0.0, 0.8454763671940566, 0.8354050728761283, 0.8096009171538925, 0.7429987752814456, 0.606110831688204, 0.05237819865629556, 0.0]\n",
      "O melhor desempenho obtido nos casos teste foi para l1_ratio = 0.5 e alpha = 200 resultando em score de 0.7697958114538056\n"
     ]
    }
   ],
   "source": [
    "#Vejamos como varia com alpha:\n",
    "scores_test = []\n",
    "scores_train = []\n",
    "best, rat, alph = 0, 0, 0\n",
    "for ratio in [0.2, 0.4, 0.5, 0.6, 0.8]:\n",
    "    for alp in [0.1, 1, 2, 5, 10, 50, 200]:\n",
    "        lasso_reg = linear_model.ElasticNet(alpha=alp, l1_ratio=ratio)\n",
    "        lasso_reg.fit(train_x, train_y)\n",
    "        score = lasso_reg.score(test_x, test_y)\n",
    "        scores_test.append(score)\n",
    "        scores_train.append(lasso_reg.score(train_x, train_y))\n",
    "        if score > best:\n",
    "            best, rat, alph = score, ratio, alp\n",
    "\n",
    "    print(f\"Para l1_ratio={ratio}:\")\n",
    "    print(scores_test)\n",
    "    print(scores_train)\n",
    "print(f\"O melhor desempenho obtido nos casos teste foi para l1_ratio = {rat} e alpha = {alp} resultando em score de {best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d844f9d",
   "metadata": {},
   "source": [
    "Portanto, de fato o Elastic-Net possui um grande potencial, tendo tido como melhor score para os casos teste em l1_ratio = 0.5 e alpha = 200, com o surpreendente score de aproximadamente 0.77."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf4b7d",
   "metadata": {},
   "source": [
    "### 4. Discussão\n",
    "Ambos os modelos de regressão L1 e L2 se provaram muito eficazes em melhorar facilmente e significativamente a acuracia do modelo de regressão linear. Em comparação com a regressão linear tradicional, que obteve score de 0.72, obtivemos rapidamente score de 0.77 usando o Elastic-Net. A variação entre os alpha usados e no caso do Elastic-Net a proporção L1/L2 demonstraram alterar significativamente o score final, sendo importante portanto atenção e teste dos valores usados para maximizar a potência dos regularizadores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219abfb0",
   "metadata": {},
   "source": [
    "### 5. Conclusão\n",
    "Os modelos Lasso, Ridge e Elastic-Net se provaram ferramentas muito úteis para melhorar a acuracia de modelos de regressão linear, sendo ferramentas agora indispensáveis a elas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d5d11",
   "metadata": {},
   "source": [
    "### 6. Referências\n",
    "DATACAMP. Tutorial: Regressão Lasso e Ridge no Python. 2025. Disponível em: https://www.datacamp.com/pt/tutorial/tutorial-lasso-ridge-regression. Acesso em: 27 set. 2025.\n",
    "\n",
    "PEDREGOSA, F. et al. sklearn.linear_model.ElasticNet. Scikit-learn, 2025. Disponível em: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html. Acesso em: 27 set. 2025.\n",
    "\n",
    "THE PANDAS DEVELOPMENT TEAM. pandas.DataFrame.drop. Pandas, 2025. Disponível em: https://pandas.pydata-org.translate.goog/docs/reference/api/pandas.DataFrame.drop.html?_x_tr_sl=en&_x_tr_tl=pt&_x_tr_hl=pt&_x_tr_pto=tc. Acesso em: 27 set. 2025.\n",
    "\n",
    "ZINI, Fernando. Regressão Linear: Teoria e Prática. YouTube, 10 set. 2023. Disponível em: https://www.youtube.com/watch?v=VqKq78PVO9g. Acesso em: 27 set. 2025.\n",
    "\n",
    "PEDREGOSA, F. et al. 1.1. Linear Models. Scikit-learn, 2025. Disponível em: https://scikit-learn.org/stable/modules/linear_model.html. Acesso em: 27 set. 2025.\n",
    "\n",
    "DeepSeek. Disponível em: https://chat.deepseek.com/share/j7z37nakhld1e4jftp. Acesso em: 27 set. 2025.\n",
    "\n",
    "\n",
    "KAGGLE. Student Academic Performance Trends Dataset. 2024. Disponível em: https://www.kaggle.com/datasets/emanfatima2025/student-academic-performance-trends"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
